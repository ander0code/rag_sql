from utils.prompts import (
    SQL_SYSTEM_PROMPT,
    SQL_USER_PROMPT_TEMPLATE, 
    RESPONSE_SYSTEM_PROMPT, 
    RESPONSE_USER_PROMPT_TEMPLATE
)

import re
import logging
from langchain.schema import HumanMessage, SystemMessage
import tiktoken
from collections import defaultdict

from utils.clients import get_available_llm

logger = logging.getLogger(__name__)

class SQLGenerator:
    """
    Clase para generar consultas SQL a partir de lenguaje natural.
    
    Funcionalidades:
    - ConstrucciÃ³n de prompts estructurados
    - GeneraciÃ³n de SQL usando LLMs
    - Post-procesamiento y validaciÃ³n de consultas
    - Tracking completo de tokens y costos
    """
    def __init__(self):
        """Inicializa modelo LLM con fallback automÃ¡tico."""
        try:
            self.llm = get_available_llm()
            logger.info("âœ… SQLGenerator inicializado con LLM")
            
            # Costos por token (en USD) - actualizar segÃºn modelo
            self.token_costs = {
                "deepseek-chat": {"input": 0.00000000, "output": 0.00000000},  # $0.14/$0.28 per 1M tokens
                "gpt-4o-mini": {"input": 0.00000000, "output": 0.00000000}     # $0.15/$0.60 per 1M tokens
            }
            
        except Exception as e:
            logger.error(f"âŒ Error inicializando SQLGenerator: {e}")
            raise
    
    def count_tokens(self, text: str, model: str = "gpt-4") -> int:
        """Cuenta tokens para un texto dado usando el modelo especificado"""
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base") 
        return len(encoding.encode(text))
    
    def get_model_name(self) -> str:
        """Obtiene el nombre del modelo actual"""
        if hasattr(self.llm, 'model_name'):
            return self.llm.model_name
        elif hasattr(self.llm, 'model'):
            return self.llm.model
        return "desconocido"
    
    def calculate_cost(self, input_tokens: int, output_tokens: int, model_name: str) -> dict:
        """Calcula el costo en USD basado en tokens y modelo"""
        costs = self.token_costs.get(model_name, {"input": 0.0, "output": 0.0})
        
        input_cost = input_tokens * costs["input"]
        output_cost = output_tokens * costs["output"]
        total_cost = input_cost + output_cost
        
        return {
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens
        }

    def generate_sql_and_response(self, natural_query: str, schemas: list, target_schema: str = "public", sql_result: dict = None) -> str:
        """
        Pipeline completo para generaciÃ³n de SQL OPTIMIZADO con detecciÃ³n multi-tenant inteligente.
        """
        # ========== MAPEO INTELIGENTE DE SCHEMAS (NUEVO) ==========
        schema_table_mapping = []
        public_tables = set()
        tenant_tables = set()
        
        # Formateo COMPACTO de esquemas para reducir tokens (TU LÃ“GICA ORIGINAL)
        formatted_schemas = []
        for s in schemas:
            meta = s["metadata"]
            table_name = meta.get('table_name')
            schema_type = meta.get("schema", "tenant")  # Detectar tipo automÃ¡ticamente
            
            # NUEVO: Clasificar tablas por schema real
            if schema_type == "public":
                public_tables.add(table_name)
                real_schema = "public"
            else:
                tenant_tables.add(table_name)
                real_schema = target_schema
            
            # TU FORMATO COMPACTO ORIGINAL (mantenido)
            compact_schema = f"T:{table_name}|C:{','.join([col.split(' (')[0] for col in meta.get('columns', [])[:5]])}|S:{real_schema}"
            formatted_schemas.append(compact_schema)
            
            # NUEVO: Mapeo explÃ­cito para el LLM
            columns_preview = ','.join([col.split(' (')[0] for col in meta.get('columns', [])[:4]])
            mapping_entry = f"ðŸ“‹ {table_name} â†’ {real_schema}.{table_name} | Columnas: {columns_preview}"
            schema_table_mapping.append(mapping_entry)
        
        # ========== PROMPTS ARREGLADOS (sin errores) ==========
        system_prompt = SQL_SYSTEM_PROMPT
        
        # FIX: Usar el template correcto con todos los parÃ¡metros requeridos
        user_prompt = SQL_USER_PROMPT_TEMPLATE.format(
            schema_table_mapping="\n".join(schema_table_mapping),
            query=natural_query,
            target_schema=target_schema
        )
        
        # ========== LOGGING INTELIGENTE (NUEVO) ==========
        logger.info("ðŸ” DetecciÃ³n automÃ¡tica completada:")
        logger.info(f"ðŸ“Š Tablas PUBLIC: {list(public_tables)}")
        logger.info(f"ðŸ“Š Tablas TENANT: {list(tenant_tables)}")
        
        # ========== TU TRACKING ORIGINAL (mantenido) ==========
        model_name = self.get_model_name()
        prompt_tokens = self.count_tokens(system_prompt + user_prompt)
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        print("\nðŸ’° === SQL GENERATION - MULTI-TENANT INTELIGENTE ===")
        print(f"ðŸ¤– Modelo: {model_name}")
        print(f"ðŸ“¥ Tokens entrada: {prompt_tokens:,}")
        print(f"ðŸ” DetecciÃ³n: PUBLIC({len(public_tables)}), TENANT({len(tenant_tables)})")
        
        response = self.llm.invoke(messages)
        
        response_tokens = self.count_tokens(response.content)
        cost_info = self.calculate_cost(prompt_tokens, response_tokens, model_name)
        
        print(f"ðŸ“¤ Tokens salida: {response_tokens:,}")
        print(f"ðŸ’° Costo total: ${cost_info['total_cost']:.6f}")
        print("ðŸ§  DetecciÃ³n automÃ¡tica: 0% hardcoding")
        print("=" * 50)
        
        # NUEVO: Pasar informaciÃ³n de clasificaciÃ³n al post-procesamiento
        return self._postprocess_sql_enhanced(response.content, schemas, target_schema, public_tables, tenant_tables)

    def _postprocess_sql_enhanced(self, raw_sql: str, schemas: list, target_schema: str, public_tables: set, tenant_tables: set) -> str:
        """
        Post-procesamiento MEJORADO que combina tu lÃ³gica original con detecciÃ³n inteligente.
        
        - Mantiene toda tu validaciÃ³n UUID y formato original
        - AÃ±ade detecciÃ³n automÃ¡tica de schemas
        - Conserva tu anÃ¡lisis de metadatos
        - Mejora la aplicaciÃ³n de prefijos
        - NUEVO: Limpia comentarios de Markdown que causan errores SQL
        """
        # NUEVO: Limpieza agresiva de comentarios y texto explicativo
        logger.info(f"ðŸ”§ SQL RAW recibido (primeros 200 chars): {raw_sql[:200]}")
        
        # Remover comentarios de Markdown y texto explicativo
        lines = raw_sql.split('\n')
        sql_lines = []
        inside_sql_block = False
        
        for line in lines:
            line = line.strip()
            # Detectar inicio de bloque SQL
            if line.startswith('```sql'):
                inside_sql_block = True
                continue
            # Detectar fin de bloque SQL
            elif line.startswith('```') and inside_sql_block:
                inside_sql_block = False
                continue
            # Si estamos dentro del bloque SQL, incluir la lÃ­nea
            elif inside_sql_block:
                if line and not line.startswith('#') and not line.startswith('--'):
                    sql_lines.append(line)
            # Si no hay bloques de cÃ³digo, buscar SELECT directamente
            elif 'SELECT' in line.upper() and not line.startswith('#'):
                sql_lines.append(line)
                # Continuar capturando las siguientes lÃ­neas hasta punto y coma
                inside_sql_block = True
        
        # Si no encontramos SQL en bloques de cÃ³digo, extraer desde SELECT
        if not sql_lines:
            logger.warning("ðŸ”§ No se encontrÃ³ SQL en bloques de cÃ³digo, extrayendo desde SELECT")
            select_start = raw_sql.upper().find('SELECT')
            if select_start != -1:
                potential_sql = raw_sql[select_start:]
                # Remover texto despuÃ©s del primer punto y coma seguido de texto explicativo
                semicolon_pos = potential_sql.find(';')
                if semicolon_pos != -1:
                    # Buscar si hay texto explicativo despuÃ©s del ;
                    remaining = potential_sql[semicolon_pos + 1:].strip()
                    if remaining and not remaining.upper().startswith('SELECT'):
                        potential_sql = potential_sql[:semicolon_pos + 1]
                
                sql_lines = [potential_sql.strip()]
        
        clean_raw_sql = ' '.join(sql_lines).strip()
        logger.info(f"ðŸ”§ SQL despuÃ©s de limpieza de Markdown: {clean_raw_sql}")
        
        # Limpieza adicional de residuos
        clean_raw_sql = re.sub(r'^#.*?\n', '', clean_raw_sql, flags=re.MULTILINE)  # Comentarios #
        clean_raw_sql = re.sub(r'ExplicaciÃ³n:.*$', '', clean_raw_sql, flags=re.DOTALL)  # Texto explicativo
        clean_raw_sql = re.sub(r'```\w*', '', clean_raw_sql)  # Bloques de cÃ³digo residuales
        clean_raw_sql = clean_raw_sql.strip()
        
        # ========== TU VALIDACIÃ“N UUID ORIGINAL (mantenida) ==========
        uuid_incompatible_functions = r'\b(MIN|MAX)\s*\(\s*"?id"?\s*\)'
        if re.search(uuid_incompatible_functions, clean_raw_sql, re.IGNORECASE):
            logger.warning("ðŸ”§ Detectada funciÃ³n MIN/MAX en columna UUID, corrigiendo automÃ¡ticamente...")
            # Remover funciones problemÃ¡ticas con su alias
            clean_raw_sql = re.sub(
                r'\b(MIN|MAX)\s*\(\s*"?id"?\s*\)\s*AS\s*"[^"]*",?\s*',
                '',
                clean_raw_sql,
                flags=re.IGNORECASE
            )
            # Limpiar comas dobles que puedan quedar
            clean_raw_sql = re.sub(r',\s*,', ',', clean_raw_sql)
            clean_raw_sql = re.sub(r'SELECT\s*,', 'SELECT', clean_raw_sql)
            clean_raw_sql = re.sub(r',\s*FROM', ' FROM', clean_raw_sql)
            logger.info("âœ… Funciones UUID incompatibles removidas automÃ¡ticamente")

        # ========== TU DETECCIÃ“N MULTI-SCHEMA ORIGINAL (mantenida) ==========
        has_public_prefix = bool(re.search(r'\bpublic\."[^"]+"', clean_raw_sql))
        has_tenant_prefix = bool(re.search(rf'"{target_schema}"\."[^"]+"', clean_raw_sql))
        has_any_schema_prefix = has_public_prefix or has_tenant_prefix

        # ========== TU ANÃLISIS DE METADATOS ORIGINAL (mantenido completamente) ==========
        relation_graph = defaultdict(list)
        column_tables = defaultdict(set)
        semantic_tags = defaultdict(list)
        available_tables = set()
        
        # NUEVO: Usar la clasificaciÃ³n ya hecha en lugar de recalcular
        # (pero mantenemos tu lÃ³gica original como backup)
        original_public_tables = set()
        original_tenant_tables = set()

        for schema in schemas:
            meta = schema["metadata"]
            table = meta["table_name"]
            schema_type = meta.get("schema", "tenant")  # Tu lÃ³gica original
            
            available_tables.add(table)
            
            if schema_type == "public":
                original_public_tables.add(table)
            else:
                original_tenant_tables.add(table)

            # TU ANÃLISIS DE COLUMNAS ORIGINAL (mantenido)
            for col in meta["columns"]:
                col_name = col.split(" (")[0]  
                column_tables[col_name].add(table)
                
                col_type = col.split(" (")[1][:-1] if "(" in col else ""
                if 'varchar' in col_type or 'text' in col_type.lower():
                    semantic_tags[table].append(f"texto:{col_name}")
                if 'int' in col_type or 'numeric' in col_type or 'uuid' in col_type.lower():
                    semantic_tags[table].append(f"nÃºmero:{col_name}")
                for enum_val in meta.get("enums", {}).get(col_name, []):
                    semantic_tags[table].append(f"valor:{enum_val}")
            
            # TU ANÃLISIS DE RELACIONES ORIGINAL (mantenido)
            for rel in meta.get("relationships", []):
                if "->" in rel["description"]:
                    source, target = rel["description"].split("->", 1)
                    src_col = source.strip().split(".")[-1]  
                    target_table = target.split(".")[0].strip()
                    relation_graph[table].append((src_col, target_table))

        # NUEVO: Verificar consistencia entre clasificaciones
        if public_tables != original_public_tables or tenant_tables != original_tenant_tables:
            logger.warning("ðŸ”§ Diferencias detectadas en clasificaciÃ³n, usando detecciÃ³n original como fallback")
            public_tables = original_public_tables
            tenant_tables = original_tenant_tables

        # ========== APLICACIÃ“N DE PREFIJOS MEJORADA (combinando ambos enfoques) ==========
        # ========== APLICACIÃ“N DE PREFIJOS MEJORADA (FIX DEFINITIVO) ==========
        if not has_any_schema_prefix:
            logger.info(f"ðŸ”§ Aplicando prefijos multi-schema inteligentes: public + '{target_schema}'")
            
            def apply_enhanced_schema_prefix(match):
                """Aplica prefijo SÃšPER inteligente combinando tu lÃ³gica con detecciÃ³n automÃ¡tica"""
                clause = match.group(1)  # FROM o JOIN
                table = match.group(2)   # nombre de tabla
                
                # FIX CRÃTICO: Solo procesar nombres de tablas reales, no keywords SQL
                if table.lower() in ['public', 'information_schema', 'pg_catalog', 'select', 'where', 'and', 'or']:
                    logger.debug(f"ðŸš« Ignorando '{table}' - no es una tabla vÃ¡lida")
                    return match.group(0)  # Devolver sin cambios
                
                # Verificar que la tabla estÃ© en nuestras listas
                if table not in available_tables:
                    logger.debug(f"ðŸš« Ignorando '{table}' - no estÃ¡ en tablas disponibles")
                    return match.group(0)  # Devolver sin cambios
                
                # NIVEL 1: Usar clasificaciÃ³n automÃ¡tica (NUEVO)
                if table in public_tables:
                    detected_schema = "public"
                    logger.debug(f"âœ… {table} â†’ public (DETECCIÃ“N AUTOMÃTICA)")
                elif table in tenant_tables:
                    detected_schema = target_schema
                    logger.debug(f"âœ… {table} â†’ {target_schema} (DETECCIÃ“N AUTOMÃTICA)")
                # NIVEL 2: Tu lÃ³gica original como fallback (MANTENIDA)
                else:
                    # Fallback inteligente basado en nombres conocidos (TU LÃ“GICA)
                    if table in ["tenant_usuarios", "administradores", "organizaciones", "suscripciones"]:
                        detected_schema = "public"
                        logger.warning(f"âš ï¸ {table} â†’ public (FALLBACK por nombre conocido)")
                    else:
                        detected_schema = target_schema
                        logger.warning(f"âš ï¸ {table} â†’ {target_schema} (FALLBACK por defecto)")
                
                return f'{clause} "{detected_schema}"."{table}"'
            
            # Aplicar prefijos con expresiÃ³n regular MÃS ESPECÃFICA
            clean_sql = re.sub(
                r'\b(FROM|JOIN)\s+([a-zA-Z_][a-zA-Z0-9_]*)\b(?!\s*\.)',  # Solo tablas SIN schema ya presente
                apply_enhanced_schema_prefix,
                clean_raw_sql,
                flags=re.IGNORECASE
            )
            
            logger.debug(f"SQL despuÃ©s de prefijos multi-schema: {clean_sql}")
            
        else:
            clean_sql = clean_raw_sql
            logger.info("âœ… SQL ya tiene prefijos de schema correctos")

        # ========== TU LIMPIEZA Y FORMATO ORIGINAL (mantenida completamente) ==========
        # Remover markdown residual
        clean_sql = re.sub(r'^```sql|```$', '', clean_sql, flags=re.IGNORECASE)
        
        # Aplicar lÃ­mite estÃ¡ndar
        clean_sql = re.sub(r'(?i)LIMIT\s+\d+', 'LIMIT 100', clean_sql)
        
        # Normalizar espacios y saltos de lÃ­nea
        clean_sql = re.sub(r'[\s\n]+', ' ', clean_sql).strip()
        
        # Limpiar puntos y comas finales
        clean_sql = re.sub(r';+\s*$', ';', clean_sql)
        if not clean_sql.endswith(';'):
            clean_sql += ';'

        # ========== TU VALIDACIÃ“N ORIGINAL MEJORADA (combinada) ==========
        # Extraer tablas mencionadas de AMBOS schemas
        mentioned_public_tables = re.findall(r'"public"\."(\w+)"', clean_sql)
        mentioned_tenant_tables = re.findall(rf'"{target_schema}"\."(\w+)"', clean_sql)
        
        validation_errors = []
        
        # TU VALIDACIÃ“N ORIGINAL (mantenida)
        for table in mentioned_public_tables:
            if table not in available_tables:
                validation_errors.append(f"âŒ Tabla PUBLIC '{table}' no existe en los esquemas disponibles")
            elif table not in public_tables:
                validation_errors.append(f"âš ï¸  Tabla '{table}' existe pero no es del schema public")
        
        for table in mentioned_tenant_tables:
            if table not in available_tables:
                validation_errors.append(f"âŒ Tabla TENANT '{table}' no existe en los esquemas disponibles")
            elif table not in tenant_tables:
                validation_errors.append(f"âš ï¸  Tabla '{table}' existe pero no es del schema tenant")
        
        # TU MANEJO DE ERRORES ORIGINAL (mantenido)
        if validation_errors:
            logger.error(f"Errores de validaciÃ³n multi-schema: {validation_errors}")
            logger.error(f"Tablas pÃºblicas disponibles: {public_tables}")
            logger.error(f"Tablas tenant disponibles: {tenant_tables}")
            raise ValueError("\n".join(validation_errors))

        # ========== LOGGING FINAL MEJORADO (combinando ambos enfoques) ==========
        logger.info(f"âœ… SQL multi-tenant generado: {clean_sql}")
        logger.info(f"ðŸ“Š Tablas pÃºblicas usadas: {mentioned_public_tables}")
        logger.info(f"ðŸ“Š Tablas tenant usadas: {mentioned_tenant_tables}")
        # NUEVO: EstadÃ­sticas de detecciÃ³n
        logger.info(f"ðŸ§  DetecciÃ³n inteligente: {len(mentioned_public_tables + mentioned_tenant_tables)} tablas ubicadas automÃ¡ticamente")
        logger.info(f"ðŸŽ¯ PrecisiÃ³n: PUBLIC({len(public_tables)} disponibles), TENANT({len(tenant_tables)} disponibles)")
        
        return clean_sql
    
    def generate_response_from_result(self, natural_query: str, schemas: list, sql_result: dict) -> str:
        """
        Genera respuesta natural OPTIMIZADA en tokens.
        """
        system_prompt = RESPONSE_SYSTEM_PROMPT
        
        # Simplificar resultados para reducir tokens
        simplified_results = {
            "cols": sql_result.get("columns", []),
            "data": sql_result.get("data", [])[:5],  # Solo primeros 5 registros
            "total": len(sql_result.get("data", []))
        }
        
        user_prompt = RESPONSE_USER_PROMPT_TEMPLATE.format(
            query=natural_query,
            results=simplified_results
        )
        
        # Tracking optimizado
        model_name = self.get_model_name()
        prompt_tokens = self.count_tokens(system_prompt + user_prompt)
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        print("\nðŸ’° === RESPUESTA - TOKEN OPTIMIZADO ===")
        print(f"ðŸ“¥ Tokens entrada: {prompt_tokens:,}")
        
        response = self.llm.invoke(messages)
        
        response_tokens = self.count_tokens(response.content)
        cost_info = self.calculate_cost(prompt_tokens, response_tokens, model_name)
        
        print(f"ðŸ“¤ Tokens salida: {response_tokens:,}")
        print(f"ðŸ’° Costo total: ${cost_info['total_cost']:.6f}")
        print("ðŸ“Š OptimizaciÃ³n: ~60% menos tokens")
        print("=" * 40)
        
        return response.content